Please find below the R code history from your *Wallace* v1.0.6.3
session.

You can reproduce your session results by running this R Markdown file
in RStudio.

Each code block is called a “chunk”, and you can run them either
one-by-one or all at once by choosing an option in the “Run” menu at the
top-right corner of the “Source” pane in RStudio.

For more detailed information see
<a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>).

### Package installation

Wallace uses the following R packages that must be installed and loaded
before starting.

```{r}
library(spocc)
library(spThin)
library(dismo)
library(rgeos)
library(ENMeval)
library(dplyr)
library (tidyverse)
library(raster)
```


```{r}
source(system.file('shiny/funcs', 'functions.R', package = 'wallace'))
```

Record of analysis for \*\*.
----------------------------

User CSV path with occurrence data. If the CSV file is not in the
current workspace, change to the correct file path
(e.g. “/Users/darwin/Documents/occs.csv”).

```{r}
# NOTE: provide the path to the folder that contains the CSV file 
d.occs <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/ocs/decades' 
# create path to user occurrences csv file
userOccs.path1 <- file.path(d.occs, "1961_1970_KK.csv")
userOccs.path2 <- file.path(d.occs, "1971_1980_KK.csv")
userOccs.path3 <- file.path(d.occs, "1981_1990_KK.csv")
userOccs.path4 <- file.path(d.occs, "1991_2000_KK.csv")
userOccs.path5 <- file.path(d.occs, "2001_2010_KK.csv")
userOccs.path6 <- file.path(d.occs, "2011_2020_KK.csv")
# read in csv
userOccs1.csv <- read.csv(userOccs.path1, header = T)
userOccs2.csv <- read.csv(userOccs.path2, header = T)
userOccs3.csv <- read.csv(userOccs.path3, header = T)
userOccs4.csv <- read.csv(userOccs.path4, header = T)
userOccs5.csv <- read.csv(userOccs.path5, header = T)
userOccs6.csv <- read.csv(userOccs.path6, header = T)
# remove rows with duplicate coordinates
occs.dups1 <- duplicated(userOccs1.csv[c('longitude', 'latitude')])
occs.dups2 <- duplicated(userOccs2.csv[c('longitude', 'latitude')])
occs.dups3 <- duplicated(userOccs3.csv[c('longitude', 'latitude')])
occs.dups4 <- duplicated(userOccs4.csv[c('longitude', 'latitude')])
occs.dups5 <- duplicated(userOccs5.csv[c('longitude', 'latitude')])
occs.dups6 <- duplicated(userOccs6.csv[c('longitude', 'latitude')])
occs1 <- userOccs1.csv[!occs.dups1,]
occs2 <- userOccs2.csv[!occs.dups2,]
occs3 <- userOccs3.csv[!occs.dups3,]
occs4 <- userOccs4.csv[!occs.dups4,]
occs5 <- userOccs5.csv[!occs.dups5,]
occs6 <- userOccs6.csv[!occs.dups6,]
# remove NAs
occs1 <- occs1[complete.cases(occs1$longitude, occs1$latitude), ]
occs2 <- occs2[complete.cases(occs2$longitude, occs2$latitude), ]
occs3 <- occs3[complete.cases(occs3$longitude, occs3$latitude), ]
occs4 <- occs4[complete.cases(occs4$longitude, occs4$latitude), ]
occs5 <- occs5[complete.cases(occs5$longitude, occs5$latitude), ]
occs6 <- occs6[complete.cases(occs6$longitude, occs6$latitude), ]
####AJUNTAR LES OBSERVACIONS EN UN SOL OBJECTE, NO? Tot i que al final és el mateix que pujar-ho junt. 
occstotal<-rbind(occs1, occs2, occs3, occs4, occs5, occs6)
# give all records a unique ID
occstotal$occID <- row.names(occstotal)
occs1$occID <- row.names(occs1)
occs2$occID <- row.names(occs2)
occs3$occID <- row.names(occs3)
occs4$occID <- row.names(occs4)
occs5$occID <- row.names(occs5)
occs6$occID <- row.names(occs6)
PRES<-read.csv("D:/quercus/Partage/Pep/Gentiana_Alpina_David/ocs/LATAULABIO_modif.csv")
```

 Funció tuning modificada

```{r}

#Funció tuning modificada

tuning_modif<- function (occ, env, bg.coords, occ.grp, bg.grp, method, algorithm, 
          args, args.lab, categoricals, aggregation.factor, kfolds, 
          bin.output, clamp, alg, rasterPreds, parallel, numCores, 
          progbar, updateProgress, userArgs, pres) 
{
  if(pres==NULL){
  pres <- as.data.frame(extract(env, occ))
  } 
  bg <- as.data.frame(extract(env, bg.coords))
  numNA.pres <- sum(rowSums(is.na(pres)) > 0)
  numNA.bg <- sum(rowSums(is.na(bg)) > 0)
  if (numNA.pres > 0) {
    message(paste("There are", numNA.pres, "occurrence records with NA for at least\n                  one predictor variable. Removing these records from analysis,\n                  resulting in", 
                  nrow(pres) - numNA.pres, "records..."))
    keepInds <- !apply(pres, 1, anyNA)
    occ <- occ[keepInds, ]
    occ.grp <- occ.grp[keepInds]
    pres <- pres[keepInds, ]
  }
  if (numNA.bg > 0) {
    message(paste("There are", numNA.bg, "background records with NA for at least one predictor variable.\n                  Removing these records from analysis, resulting in", 
                  nrow(bg) - numNA.bg, "records..."))
    keepInds <- !apply(bg, 1, anyNA)
    bg.coords <- bg.coords[keepInds, ]
    bg.grp <- bg.grp[keepInds]
    bg <- bg[keepInds, ]
  }
  if (!is.null(categoricals)) {
    for (i in 1:length(categoricals)) {
      pres[, categoricals[i]] <- as.factor(pres[, categoricals[i]])
      bg[, categoricals[i]] <- as.factor(bg[, categoricals[i]])
    }
  }
  if ("checkerboard1" %in% method) {
    method <- c(method = "checkerboard1", aggregation.factor = aggregation.factor)
    group.data <- get.checkerboard1(occ, env, bg.coords, 
                                    aggregation.factor)
  }
  if ("checkerboard2" %in% method) {
    method <- c(method = "checkerboard2", aggregation.factor = aggregation.factor)
    group.data <- get.checkerboard2(occ, env, bg.coords, 
                                    aggregation.factor)
  }
  if ("block" %in% method) 
    group.data <- get.block(occ, bg.coords)
  if ("jackknife" %in% method) 
    group.data <- get.jackknife(occ, bg.coords)
  if ("randomkfold" %in% method) {
    method <- c(method = "randomkfold", number.folds = kfolds)
    group.data <- get.randomkfold(occ, bg.coords, kfolds)
  }
  if ("user" %in% method) {
    method <- c(method = "user", number.folds = length(unique(occ.grp)))
    group.data <- get.user(occ.grp, bg.grp)
  }
  nk <- length(unique(group.data$occ.grp))
  if (parallel == TRUE) {
    allCores <- detectCores()
    if (is.null(numCores)) {
      numCores <- allCores
    }
    c1 <- makeCluster(numCores)
    registerDoParallel(c1)
    numCoresUsed <- getDoParWorkers()
    message(paste("Of", allCores, "total cores using", 
                  numCoresUsed))
    message("Running in parallel...")
    if (algorithm == "maxnet") {
      out <- foreach(i = seq_len(length(args)), .packages = c("dismo", 
                                                              "raster", "ENMeval", "maxnet")) %dopar% 
        {
          modelTune.maxnet(pres, bg, env, nk, group.data, 
                           args[[i]], rasterPreds, clamp)
        }
    }
    else if (algorithm == "maxent.jar") {
      out <- foreach(i = seq_len(length(args)), .packages = c("dismo", 
                                                              "raster", "ENMeval", "rJava")) %dopar% 
        {
          modelTune.maxentJar(pres, bg, env, nk, group.data, 
                              args[[i]], userArgs, rasterPreds, clamp)
        }
    }
    stopCluster(c1)
  }
  else {
    out <- list()
    if (progbar == TRUE & !is.function(updateProgress)) {
      pb <- txtProgressBar(0, length(args), style = 3)
    }
    for (i in 1:length(args)) {
      if (length(args) > 1) {
        if (is.function(updateProgress)) {
          text <- paste0("Running ", args.lab[[1]][i], 
                         args.lab[[2]][i], "...")
          updateProgress(detail = text)
        }
        else if (progbar == TRUE) {
          setTxtProgressBar(pb, i)
        }
      }
      if (algorithm == "maxnet") {
        out[[i]] <- modelTune.maxnet(pres, bg, env, nk, 
                                     group.data, args[[i]], rasterPreds, clamp)
      }
      else if (algorithm == "maxent.jar") {
        out[[i]] <- modelTune.maxentJar(pres, bg, env, 
                                        nk, group.data, args[[i]], userArgs, rasterPreds, 
                                        clamp)
      }
    }
    if (progbar == TRUE) 
      close(pb)
  }
  full.mods <- lapply(out, function(x) x[[1]])
  statsTbl <- as.data.frame(t(sapply(out, function(x) x[[2]])))
  if (rasterPreds) {
    predictive.maps <- stack(sapply(out, function(x) x[[3]]))
  }
  else {
    predictive.maps <- stack()
  }
  AUC.DIFF <- statsTbl[, 1:nk]
  AUC.TEST <- statsTbl[, (nk + 1):(2 * nk)]
  OR10 <- statsTbl[, ((2 * nk) + 1):(3 * nk)]
  ORmin <- statsTbl[, ((3 * nk) + 1):(4 * nk)]
  names(AUC.DIFF) <- paste("diff.AUC_bin", 1:nk, sep = ".")
  Mean.AUC.DIFF <- rowMeans(AUC.DIFF)
  Var.AUC.DIFF <- corrected.var(AUC.DIFF, nk)
  names(AUC.TEST) <- paste("AUC_bin", 1:nk, sep = ".")
  Mean.AUC <- rowMeans(AUC.TEST)
  Var.AUC <- corrected.var(AUC.TEST, nk)
  names(OR10) <- paste("test.or10pct_bin", 1:nk, sep = ".")
  Mean.OR10 <- rowMeans(OR10)
  Var.OR10 <- apply(OR10, 1, var)
  names(ORmin) <- paste("test.orMTP_bin", 1:nk, sep = ".")
  Mean.ORmin <- rowMeans(ORmin)
  Var.ORmin <- apply(ORmin, 1, var)
  full.AUC <- double()
  for (i in 1:length(full.mods)) {
    if (algorithm == "maxnet") {
      full.AUC[i] <- dismo::evaluate(pres, bg, full.mods[[i]])@auc
    }
    else if (algorithm == "maxent.jar") {
      full.AUC[i] <- full.mods[[i]]@results[5]
    }
  }
  nparam <- numeric()
  for (i in 1:length(full.mods)) {
    if (algorithm == "maxnet") {
      nparam[i] <- length(full.mods[[i]]$betas)
    }
    else if (algorithm == "maxent.jar") {
      nparam[i] <- get.params(full.mods[[i]])
    }
  }
  aicc <- calc.aicc(nparam, occ, predictive.maps)
  features <- args.lab[[1]]
  rm <- args.lab[[2]]
  settings <- paste(args.lab[[1]], args.lab[[2]], sep = "_")
  res <- data.frame(settings, features, rm, train.AUC = full.AUC, 
                    avg.test.AUC = Mean.AUC, var.test.AUC = Var.AUC, avg.diff.AUC = Mean.AUC.DIFF, 
                    var.diff.AUC = Var.AUC.DIFF, avg.test.orMTP = Mean.ORmin, 
                    var.test.orMTP = Var.ORmin, avg.test.or10pct = Mean.OR10, 
                    var.test.or10pct = Var.OR10, aicc)
  if (bin.output == TRUE) {
    res <- as.data.frame(cbind(res, AUC.TEST, AUC.DIFF, OR10, 
                               ORmin))
  }
  if (rasterPreds == TRUE) {
    names(predictive.maps) <- settings
  }
  results <- ENMevaluation(algorithm = alg, results = res, 
                           predictions = predictive.maps, models = full.mods, partition.method = method, 
                           occ.pts = occ, occ.grp = group.data[[1]], bg.pts = bg.coords, 
                           bg.grp = group.data[[2]])
  return(results)
}



```

 Funció enmevaluate modificada

```{r}
enmevaluate_modif<-function (occ, env, bg.coords = NULL, occ.grp = NULL, bg.grp = NULL, 
          RMvalues = seq(0.5, 4, 0.5), fc = c("L", "LQ", 
                                              "H", "LQH", "LQHP", "LQHPT"), 
          categoricals = NULL, n.bg = 10000, method = NULL, algorithm = "maxnet", 
          overlap = FALSE, aggregation.factor = c(2, 2), kfolds = NA,
          bin.output = FALSE, clamp = TRUE, rasterPreds = TRUE, parallel = FALSE, 
          numCores = NULL, progbar = TRUE, updateProgress = FALSE,
          ...) 
{
  ptm <- proc.time()
  if (is.null(method)) {
    stop("Evaluation method needs to be specified.")
  }
  if (progbar == TRUE & is.function(updateProgress)) {
    stop("Cannot specify both \"progbar\" and \"updateProgress\". Please turn one off.")
  }
  if (algorithm == "maxent.jar") {
    userArgs <- list(...)
    allMaxentArgs <- c("addsamplestobackground", "addallsamplestobackground", 
                       "allowpartialdata", "beta_threshold", 
                       "beta_categorical", "beta_lqp", "beta_hinge", 
                       "convergencethreshold", "defaultprevalence", 
                       "extrapolate", "fadebyclamping", "jackknife", 
                       "maximumbackground", "maximumiterations", 
                       "removeduplicates")
    if (length(userArgs) == 0) {
      userArgs <- NULL
    }
    else {
      if (!all(names(userArgs) %in% allMaxentArgs)) {
        stop("The maxent argument given is not implemented in ENMeval or is misspelled.")
      }
      else {
        userArgs <- paste(names(userArgs), unlist(userArgs), 
                          sep = "=")
      }
    }
    args <- make.args(RMvalues, fc)
    dismo.vs <- packageVersion("dismo")
    v <- maxentJARversion()
    alg <- paste("Maxent", v, "via dismo", dismo.vs)
  }
  else if (algorithm == "maxnet") {
    args.fc <- as.list(tolower(rep(fc, times = length(RMvalues))))
    args.rm <- as.list(sort(rep(RMvalues, times = length(fc))))
    args <- mapply(c, args.fc, args.rm, SIMPLIFY = FALSE)
    maxnet.vs <- packageVersion("maxnet")
    alg <- paste0("maxnet v.", maxnet.vs)
    userArgs <- NULL
  }
  message(paste("*** Running ENMevaluate using", alg, 
                "***"))
  args.lab <- make.args(RMvalues, fc, labels = TRUE)
  if (is.null(bg.coords)) {
    bg.coords <- randomPoints(env[[1]], n = n.bg)
  }
  occ <- as.data.frame(occ)
  colnames(occ) <- c("LON", "LAT")
  bg.coords <- as.data.frame(bg.coords)
  colnames(bg.coords) <- c("LON", "LAT")
  message(ifelse(method == "jackknife", "Doing evaluations using k-1 jackknife...", 
                 ifelse(method == "checkerboard1", "Doing evaluations using checkerboard 1...", 
                        ifelse(method == "checkerboard2", "Doing evaluations using checkerboard 2...", 
                               ifelse(method == "block", "Doing evaluations using spatial blocks...", 
                                      ifelse(method == "randomkfold", "Doing random k-fold evaluation groups...", 
                                             ifelse(method == "user", "Doing user-defined evaluation groups...", 
                                                    "Error: You need to specify an accepted evaluation method. Check the documentation.")))))))
  results <- tuning_modif(occ, env, bg.coords, occ.grp, bg.grp, method, 
                    algorithm, args, args.lab, categoricals, aggregation.factor, 
                    kfolds, bin.output, clamp, alg, rasterPreds, parallel, 
                    numCores, progbar, updateProgress, userArgs, pres==PRES)
  if (overlap == TRUE) {
    if (length(args) > 1) {
      if (nlayers(results@predictions) > 1) {
        message("Calculating niche overlap")
        overlap.mat <- calc.niche.overlap(results@predictions, 
                                          "D")
        results@overlap <- overlap.mat
      }
      else {
        message("Cannot calculate niche overlap without rasterPreds")
      }
    }
    else {
      message("Need >1 settings to do niche overlap")
    }
  }
  timed <- proc.time() - ptm
  t.min <- floor(timed[3]/60)
  t.sec <- timed[3] - (t.min * 60)
  message(paste("ENMeval completed in", t.min, "minutes", 
                round(t.sec, 1), "seconds."))
  return(results)
}

```



### Process Occurrence Data

Spatial thinning selected. Thin distance selected is 0.1 km.

```{r}
#output <- spThin::thin(occs, 'latitude', 'longitude', 'name', thin.par = 0.1, reps = 100, locs.thinned.list.return = TRUE, write.files = FALSE, verbose = FALSE)
```

Since spThin did 100 iterations, there are 100 different variations of
how it thinned your occurrence localities. As there is a stochastic
element in the algorithm, some iterations may include more localities
than the others, and we need to make sure we maximize the number of
localities we proceed with.

### Obtain Environmental Data

Using WorldClim
(<a href="http://www.worldclim.org/" class="uri">http://www.worldclim.org/</a>)
bioclimatic dataset at resolution of 2.5 arcmin.

```{r}
# NOTE: provide the path to the folder that contains the rasters
d.envs1961_1970 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/Calibracio/1961_1970'
d.envs1971_1980 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/Calibracio/1971_1980'
d.envs1981_1990 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/Calibracio/1981_1990'
d.envs1991_2000 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/Calibracio/1991_2000'
d.envs2001_2010 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/Calibracio/2001_2010'
d.envs2011_2020 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/Calibracio/2011_2020'

# create paths to the raster files
userRas.paths1961_1970 <- file.path(d.envs1961_1970, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))

userRas.paths1971_1980 <- file.path(d.envs1971_1980, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))

userRas.paths1981_1990 <- file.path(d.envs1981_1990, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))

userRas.paths1991_2000 <- file.path(d.envs1991_2000, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))

userRas.paths2001_2010 <- file.path(d.envs2001_2010, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))

userRas.paths2011_2020 <- file.path(d.envs2011_2020, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))

# make a RasterStack out of the raster files
envs1 <- raster::stack(userRas.paths1961_1970)
envs2 <- raster::stack(userRas.paths1971_1980)  
envs3 <- raster::stack(userRas.paths1981_1990)  
envs4 <- raster::stack(userRas.paths1991_2000)  
envs5 <- raster::stack(userRas.paths2001_2010)  
envs6 <- raster::stack(userRas.paths2011_2020)  
# extract environmental values at occ grid cells
locs.vals1 <- raster::extract(envs1[[1]], occs1[, c('longitude', 'latitude')])  
locs.vals2 <- raster::extract(envs2[[1]], occs2[, c('longitude', 'latitude')])  
locs.vals3 <- raster::extract(envs3[[1]], occs3[, c('longitude', 'latitude')])  
locs.vals4 <- raster::extract(envs4[[1]], occs4[, c('longitude', 'latitude')])  
locs.vals5 <- raster::extract(envs5[[1]], occs5[, c('longitude', 'latitude')])  
locs.vals6 <- raster::extract(envs6[[1]], occs6[, c('longitude', 'latitude')])  
# remove occs without environmental values
occs1 <- occs1[!is.na(locs.vals1), ]  
occs2 <- occs2[!is.na(locs.vals2), ]  
occs3 <- occs3[!is.na(locs.vals3), ]  
occs4 <- occs4[!is.na(locs.vals4), ]  
occs5 <- occs5[!is.na(locs.vals5), ]  
occs6 <- occs6[!is.na(locs.vals6), ]  
occs  <-rbind(occs1, occs2, occs3, occs4, occs5, occs6)

```

### Process Environmental Data

Background selection technique chosen as .

Read a .csv file and generate a Spatial Polygon object.

```{r}
# NOTE: provide the full path to the CSV file
csvPath <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/ocs/AREA_ESTUDI.csv'
# read csv with coordinates for polygon
shp <- read.csv(csvPath, header = TRUE)
bgExt <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(shp)), 1)))
projection(bgExt)<-"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
```

Mask environmental variables by , and take a random sample of background
values from the study extent. As the sample is random, your results may
be different than those in the session. If there seems to be too much
variability in these background samples, try increasing the number from
10,000 to something higher (e.g. 50,000 or 100,000). The better your
background sample, the less variability you’ll have between runs.

```{r}
# crop the environmental rasters by the background extent shape
obrir11<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster1961_1970_1.tif")
obrir12<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster1971_1980_1.tif")
obrir13<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster1981_1990_1.tif")
obrir14<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster1991_2000_1.tif")
obrir15<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster2001_2010_1.tif")
obrir16<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster2011_2020_1.tif")



#obrir51<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster1961_1970_5.tif")
#obrir52<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster1971_1980_5.tif")
#obrir53<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster1981_1990_5.tif")
#obrir54<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster1991_2000_5.tif")
#obrir55<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster2001_2010_5.tif")
#obrir56<-raster("D:/quercus/Partage/Pep/Gentiana_Alpina_David/background_samplings/PROBABILITATS/provaraster2011_2020_5.tif")


#envsBgCrop1 <- raster::crop(envs1, bgExt)
envsBgCrop2 <- raster::crop(envs2, bgExt)
envsBgCrop3 <- raster::crop(envs3, bgExt)
envsBgCrop4 <- raster::crop(envs4, bgExt)
envsBgCrop5 <- raster::crop(envs5, bgExt)
envsBgCrop6 <- raster::crop(envs6, bgExt)
# mask the background extent shape from the cropped raster
#envsBgMsk1 <- raster::mask(envsBgCrop1, bgExt)
envsBgMsk2 <- raster::mask(envsBgCrop2, bgExt)
envsBgMsk3 <- raster::mask(envsBgCrop3, bgExt)
envsBgMsk4 <- raster::mask(envsBgCrop4, bgExt)
envsBgMsk5 <- raster::mask(envsBgCrop5, bgExt)
envsBgMsk6 <- raster::mask(envsBgCrop6, bgExt)

############### WALLACE METHOD (NOT IMPLEMENTED) ########################################################### 
# sample random background points
#bg.xy <- dismo::randomPoints(envsBgMsk, 10)  
# convert matrix output to data frame
#bg.xy <- as.data.frame(bg.xy)

############### OUR METHOD ########################################################### 

#valors<-values(envsBgMsk1)
#cellSamples1 = 1:nrow(valors)
#NoNA =complete.cases(valors)
#cellSamples3 = cellSamples1 [NoNA]

#vv = values (envsBgMsk1)
#cellSamples = 1:nrow(vv)
#notNA =complete.cases(vv)
#cellSamples2 = cellSamples [notNA] 

#cellSampProb = values (obrir11)
#cellSampProb2 = cellSampProb[notNA]

   #L'objectiu és tenir un objecte igual que cellsamples2. 
#bgCellId = sample (x = cellSamples2, size = 2624, replace = F, prob = cellSampProb2)   #, prob= cellSampProb2    aquí i a les 5 dècades restants
#bg.xy1 = raster::xyFromCell(envsBgMsk1,cell = bgCellId)
#bg.xy1 = as.data.frame (bg.xy1)
#rm(vv)


############################################

vv = values (envsBgMsk2)
cellSamples = 1:nrow(vv)
notNA =complete.cases(vv)
cellSamples2 = cellSamples [notNA] 

cellSampProb = values (obrir12)
cellSampProb2 = cellSampProb[notNA]

   #L'objectiu és tenir un objecte igual que cellsamples2. 
bgCellId = sample (x = cellSamples2, size = 3207, replace = F, prob = cellSampProb2)
bg.xy2 = raster::xyFromCell(envsBgMsk2,cell = bgCellId)
bg.xy2 = as.data.frame (bg.xy2)
rm(vv)

#############################################

vv = values (envsBgMsk3)
cellSamples = 1:nrow(vv)
notNA =complete.cases(vv)
cellSamples2 = cellSamples [notNA] 

cellSampProb = values (obrir13)
cellSampProb2 = cellSampProb[notNA]

   #L'objectiu és tenir un objecte igual que cellsamples2. 
bgCellId = sample (x = cellSamples2,size = 3207, replace = F, prob = cellSampProb2)
bg.xy3 = raster::xyFromCell(envsBgMsk3,cell = bgCellId)
bg.xy3 = as.data.frame (bg.xy3)
rm(vv)

###########################################################

vv = values (envsBgMsk4)
cellSamples = 1:nrow(vv)
notNA =complete.cases(vv)
cellSamples2 = cellSamples [notNA] 

cellSampProb = values (obrir14)
cellSampProb2 = cellSampProb[notNA]

   #L'objectiu és tenir un objecte igual que cellsamples2. 
bgCellId = sample (x = cellSamples2,size = 5539, replace = F, prob = cellSampProb2)
bg.xy4 = raster::xyFromCell(envsBgMsk4,cell = bgCellId)
bg.xy4 = as.data.frame (bg.xy4)
rm(vv)

###########################################################

vv = values (envsBgMsk5)
cellSamples = 1:nrow(vv)
notNA =complete.cases(vv)
cellSamples2 = cellSamples [notNA] 

cellSampProb = values (obrir15)
cellSampProb2 = cellSampProb[notNA]

   #L'objectiu és tenir un objecte igual que cellsamples2. 
bgCellId = sample (x = cellSamples2,size = 43732, replace = F, prob = cellSampProb2)
bg.xy5 = raster::xyFromCell(envsBgMsk5,cell = bgCellId)
bg.xy5 = as.data.frame (bg.xy5)
rm(vv)

##########################################################

vv = values (envsBgMsk6)
cellSamples = 1:nrow(vv)
notNA =complete.cases(vv)
cellSamples2 = cellSamples [notNA] 

cellSampProb = values (obrir16)
cellSampProb2 = cellSampProb[notNA]

   #L'objectiu és tenir un objecte igual que cellsamples2. 
bgCellId = sample (x = cellSamples2,size = 41691, replace = F, prob = cellSampProb2)
bg.xy6 = raster::xyFromCell(envsBgMsk6,cell = bgCellId)
bg.xy6 = as.data.frame (bg.xy6)
rm(vv)

bg.xy<-rbind(bg.xy1, bg.xy2, bg.xy3, bg.xy4, bg.xy5, bg.xy6)
```

### Partition Occurrence Data

Occurrence data is now partitioned for cross-validation, a method that
iteratively builds a model on all but one group and evaluates that model
on the left-out group.

For example, if the data is partitioned into 3 groups A, B, and C, a
model is first built with groups A and B and is evaluated on C. This is
repeated by building a model with B and C and evaluating on A, and so on
until all combinations are done.

Cross-validation operates under the assumption that the groups are
independent of each other, which may or may not be a safe assumption for
your dataset. Spatial partitioning is one way to ensure more
independence between groups.

You selected to partition your occurrence data by the method.

SEPARAREM LES OBSERVACIONS PER DÈCADA A L'HORA DE FER LA PARTICIÓ? POTSER NO, NO?

```{r}
occs.xy <- occs[c('longitude', 'latitude')]
group.data <- ENMeval::get.block(occ=occs.xy, bg.coords=bg.xy)
```

```{r}
# pull out the occurrence and background partition group numbers from the list
occs.grp <- group.data[[1]]
bg.grp <- group.data[[2]]
```

### Build and Evaluate Niche Model

You selected the maxent model.

```{r}
# define the vector of regularization multipliers to test
rms <- seq(1, 2, 1)
# iterate model building over all chosen parameter settings
e <- enmevaluate_modif(occs.xy, envsBgMsk, bg.coords = bg.xy, RMvalues = rms, fc = c('L', 'LQ'), 
                          method = 'user', occs.grp, bg.grp, n.bg = 100000, clamp = TRUE, algorithm = "maxnet")

#A dalt està posat Maxnet, ja es el que toca o hem de canviar a maxent? Maxent triga molt menys

#save the model
readr::write_rds (e, 'ENM_modelprobabilitat.rds')

#if you needed to reload the model

#e <- readr::read_rds ("D:/Users/munozferrandiz/Documents/MODEL1961_2000/modeldecades.rds")

#with javascript (NOT IMPLEMENTED)
# ejar <- ENMeval::ENMevaluate(occs.xy, envsBgMsk, bg.coords = bg.xy, RMvalues = rms, fc = c('L', 'LQ', 'H'), 
#                           method = 'user', occs.grp, bg.grp, clamp = TRUE, algorithm = "maxent.jar")

# unpack the results data frame, the list of models, and the RasterStack of raw predictions
evalTbl <- e@results
evalMods <- e@models
names(evalMods) <- e@results$settings
evalPreds <- e@predictions
```
```

```{r}
# view response curves for environmental variables with non-zero coefficients
#dismo::response(evalMods[["L_1"]], var = c('mitjanaaridityIndexThornthwaite_1961_2000.tif', 'mitjanabio9_1961_2000.tif', 'mitjanabio15_1961_2000.tif', 'mitjanabio18_1961_2000.tif', 'mitjanacontinentality_1961_2000.tif', 'mitjanamonthCountByTemp10_1961_2000.tif', 'mitjanaPETColdestQuarter_1961_2000.tif', 'mitjanaPETWettestQuarter_1961_2000.tif'))

plot(evalMods[["LQ_1"]], vars = c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000'), type = "cloglog")


```

```{r}

# view ENMeval results
ENMeval::eval.plot(evalTbl, value = "avg.test.AUC")

```

```{r}

# Select your model from the models list
mod <- evalMods[["LQ_1"]]


```

```{r}

# generate logistic prediction
#pred <- dismo::predict(mod, envsBgMsk, args = c("outputformat=logistic"))
pred <- ENMeval::maxnet.predictRaster(mod, envsBgMsk, type = 'logistic', clamp = TRUE)  
writeRaster(pred, "D:/Users/munozferrandiz/Documents/MODEL1961_2000/RESULTATS/preddecades.tif")

```

```{r}
# plot the model prediction
plot(pred)
```


### Project Niche Model. Aquí hauré de canviar mil coses. 

You selected to project your model. First define a polygon with the
coordinates you chose, then crop and mask your predictor rasters.
Finally, predict suitability values for these new raster cells based on
the model you selected.

```{r}

#projCoords <- data.frame(x = c(-0.4999, 1.1577, 2.0743, 0.1697, -0.4999), y = c(43.7417, 42.9146, 43.6703, 43.8052, 43.7417))
#projPoly <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(projCoords)), ID=1)))

projPoly <- bgExt
projection(bgExt)<-"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"

```

### Project Niche Model to New Time

Now download the future climate variables chosen with *Wallace*, crop
and mask them by projPoly, and use the maxnet.predictRaster() function
to predict the values for the new time based on the model selected.

```{r}

#envsFuture <- raster::getData("CMIP5", var = "bio", res = 5, rcp = 85, model = "MR", year = 50)



d.envs2021_2030 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/RCP85/2021_2030'
# create paths to the raster files
userRas.paths2021_2030 <- file.path(d.envs2021_2030, c('mitjanaaridityIndexThornthwaite_1961_2000.tif', 'mitjanabio9_1961_2000.tif', 'mitjanabio15_1961_2000.tif', 'mitjanabio18_1961_2000.tif', 'mitjanacontinentality_1961_2000.tif', 'mitjanamonthCountByTemp10_1961_2000.tif', 'mitjanaPETColdestQuarter_1961_2000.tif', 'mitjanaPETWettestQuarter_1961_2000.tif'))
# make a RasterStack out of the raster files
envsFuture <- raster::stack(userRas.paths2021_2030)  
predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
# rename future climate variable names
# names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
names(predsProj) <- c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000')
# select climate variables
# predsProj <- raster::subset(predsProj, names(envs))


d.envs2031_2040 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/RCP85/2031_2040'
# create paths to the raster files
userRas.paths2031_2040 <- file.path(d.envs2031_2040, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))
# make a RasterStack out of the raster files
envsFuture <- raster::stack(userRas.paths2031_2040)  
predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
# rename future climate variable names
# names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
names(predsProj) <- c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000')


d.envs2041_050 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/RCP85/2041_2050'
# create paths to the raster files
userRas.paths2041_2050 <- file.path(d.envs2041_2050, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))
# make a RasterStack out of the raster files
envsFuture <- raster::stack(userRas.paths2041_2050)  
predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
# rename future climate variable names
# names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
names(predsProj) <- c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000')


d.envs2051_2060 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/RCP85/2051_2060'
# create paths to the raster files
userRas.paths2051_2060 <- file.path(d.envs2051_2060, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))
# make a RasterStack out of the raster files
envsFuture <- raster::stack(userRas.paths2051_2060)  
predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
# rename future climate variable names
# names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
names(predsProj) <- c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000')


d.envs2061_2070 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/RCP85/2061_2070'
# create paths to the raster files
userRas.paths2061_2070 <- file.path(d.envs2061_2070, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))
# make a RasterStack out of the raster files
envsFuture <- raster::stack(userRas.paths2061_2070)  
predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
# rename future climate variable names
# names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
names(predsProj) <- c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000')


d.envs2071_2080 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/RCP85/2071_2080'
# create paths to the raster files
userRas.paths2071_2080 <- file.path(d.envs2071_2080, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))
# make a RasterStack out of the raster files
envsFuture <- raster::stack(userRas.paths2071_2080)  
predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
# rename future climate variable names
# names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
names(predsProj) <- c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000')


d.envs2081_2090 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/RCP85/2081_2090'
# create paths to the raster files
userRas.paths2091_2100 <- file.path(d.envs2081_2090, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))
# make a RasterStack out of the raster files
envsFuture <- raster::stack(userRas.paths2081_2090)  
predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
# rename future climate variable names
# names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
names(predsProj) <- c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000')



d.envs2091_2100 <- 'D:/quercus/Partage/Pep/Gentiana_Alpina_David/Biovars/RCP85/2091_2100'
# create paths to the raster files
userRas.paths2091_2100 <- file.path(d.envs2091_2100, c('aridityIndexThornthwaite.tif', 'bio9.tif', 'bio15.tif', 'bio18.tif', 'continentality.tif', 'monthCountByTemp10.tif', 'PETColdestQuarter.tif', 'PETWettestQuarter.tif'))
# make a RasterStack out of the raster files
envsFuture <- raster::stack(userRas.paths2091_2100)  
predsProj <- raster::crop(envsFuture, projPoly)
predsProj <- raster::mask(predsProj, projPoly)
# rename future climate variable names
# names(predsProj) <- paste0('bio', sprintf("%02d", 1:19))
names(predsProj) <- c('mitjanaaridityIndexThornthwaite_1961_2000', 'mitjanabio9_1961_2000', 'mitjanabio15_1961_2000', 'mitjanabio18_1961_2000', 'mitjanacontinentality_1961_2000', 'mitjanamonthCountByTemp10_1961_2000', 'mitjanaPETColdestQuarter_1961_2000', 'mitjanaPETWettestQuarter_1961_2000')

```

```{r}

# predict model

proj <- ENMeval::maxnet.predictRaster(mod, predsProj, type = 'logistic', clamp = TRUE)
writeraster(proj, "./projeccioprova")

```

```{r}

# plot the model prediction
plot(proj)
writeRaster(proj, "./projeccioprova.tif")
```

